name: llm-qa-indexer
type: job
env:
  OPENAI_API_KEY: <openai api key>
  ML_REPO_NAME: <ml repo name>
image:
  type: build
  build_spec:
    type: dockerfile
    command: >-
      python -m backend.indexer.main --collection_name {{collection_name}} --chunk_size {{chunk_size}} --indexer_job_run_name {{indexer_job_run_name}} --knowledge_source  {{knowledge_source}} --embedder_config {{embedder_config}} --parser_config {{parser_config}} --vector_db_config {{vector_db_config}}
    dockerfile_path: ./backend/Dockerfile
    build_context_path: ./
  build_source:
    type: local
    local_build: false
params:
  - name: collection_name
    default: docs-qa-llm
    param_type: string
  - name: chunk_size
    param_type: string
  - name: indexer_job_run_name
    param_type: string
  - name: knowledge_source
    param_type: string
  - name: embedder_config
    default: >-
      {"provider": "OpenAI", "config": {"model": "text-embedding-ada-002"}}
    param_type: string
  - name: parser_config
    default: >-
      {".md": "MarkdownParser", ".pdf": "PdfParserFast", ".txt": "TextParser"}
    param_type: string
    description: Map of file type to parsers
  - name: vector_db_config
    default: >-
        {"provider": "weaviate", "url": ""}
    param_type: string
    description: Vector database config
retries: 0
trigger:
  type: manual
resources:
  node:
    type: node_selector
    capacity_type: on_demand
  cpu_request: 2
  cpu_limit: 4
  memory_request: 2000
  memory_limit: 4000
  ephemeral_storage_request: 4000
  ephemeral_storage_limit: 8000
