# Using hosted qdrant and truefoundry
METADATA_STORE_CONFIG='{"provider":"truefoundry","config":{"ml_repo_name":"<your_ml_repo_name>"}}'
VECTOR_DB_CONFIG='{"provider":"qdrant","url":"<your_qdrant_url>"}'

# Should be set to the root path of the service
TFY_SERVICE_ROOT_PATH='/'


# Used to authenticate with TF LLM Gateway, and other functionalities like MLRepo
TFY_API_KEY="<your_truefoundry_api_key>"
TFY_HOST="<your_truefoundry_dashbaord_url>"

# If you are using OpenAI models and not using TF LLM Gateway
# Need to provide an OpenAI API key
OPENAI_API_KEY="<your_openai_api_key>"


# Optional if you want to use redis for caching
EMBEDDING_CACHE_CONFIG='{"provider":"redis","url":"<your_redis_url>"}'

LOG_LEVEL="INFO"

# Optional if you have indexer job deployed in truefoundry, and you want to trigger it for indexing.
JOB_FQN="<your_job_fqn>"
JOB_COMPONENT_NAME="<your_job_name>"

# If using Local models
LOCAL=true
